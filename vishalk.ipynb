{
 "cells": [
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-27T14:03:47.979969Z",
     "start_time": "2025-07-27T14:03:47.974038Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms,models\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T14:03:48.043150Z",
     "start_time": "2025-07-27T14:03:48.038118Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trans = transforms.Compose([\n",
    "\n",
    "    transforms.Resize(size=(256,256)),\n",
    "    transforms.Grayscale(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5],std=[0.5]),\n",
    "\n",
    "])"
   ],
   "id": "7ec64b535904998e",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T14:03:48.144952Z",
     "start_time": "2025-07-27T14:03:48.098377Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset=datasets.ImageFolder('C:/Users/visha/Downloads/archive/chest_xray/train',transform=trans)\n",
    "val_dataset = datasets.ImageFolder('C:/Users/visha/Downloads/archive/chest_xray/test',transform=trans)"
   ],
   "id": "a6c3252268bbace9",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T14:15:31.112107Z",
     "start_time": "2025-07-27T14:15:31.105699Z"
    }
   },
   "cell_type": "code",
   "source": "print(train_dataset.class_to_idx)",
   "id": "2b5f63af008d688f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NORMAL': 0, 'PNEUMONIA': 1}\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T14:03:48.221685Z",
     "start_time": "2025-07-27T14:03:48.210025Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_loader = DataLoader(train_dataset,batch_size=64,shuffle=True)\n",
    "test_loader = DataLoader(train_dataset,batch_size =64,shuffle=False)"
   ],
   "id": "467e4f09be3bd9f4",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T14:03:48.291596Z",
     "start_time": "2025-07-27T14:03:48.284487Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device('cuda' if (torch.cuda.is_available()==True) else 'cpu')\n",
    "device"
   ],
   "id": "241b291e2834297c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T14:03:48.623501Z",
     "start_time": "2025-07-27T14:03:48.404295Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "model.conv1 = nn.Conv2d(1, 64, 7, stride=2, padding=3, bias=False)\n",
    "\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs,2)"
   ],
   "id": "ba49805566faf7c",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T14:03:48.701792Z",
     "start_time": "2025-07-27T14:03:48.697443Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "875c18338646d456",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T14:03:48.786348Z",
     "start_time": "2025-07-27T14:03:48.766919Z"
    }
   },
   "cell_type": "code",
   "source": "model = model.to(device)",
   "id": "ab9034a3bbd1fa3c",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T14:03:48.847815Z",
     "start_time": "2025-07-27T14:03:48.837653Z"
    }
   },
   "cell_type": "code",
   "source": [
    "optimizer = optim.Adam(model.parameters(),lr=0.001)\n",
    "loss_function = nn.CrossEntropyLoss()"
   ],
   "id": "b675751bc8efe0b6",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T14:13:49.787Z",
     "start_time": "2025-07-27T14:03:48.902456Z"
    }
   },
   "cell_type": "code",
   "source": [
    "no_of_epochs = 5\n",
    "for e in range(no_of_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct_preds = 0\n",
    "    total_data_points = 0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        output = model(images)\n",
    "        loss = loss_function(output, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_data_points += labels.size(0)\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        predicted_vals = torch.max(output, dim=1)[1]\n",
    "        correct_preds += (predicted_vals == labels).sum().item()\n",
    "\n",
    "    epoch_accuracy = 100 * correct_preds / total_data_points\n",
    "    epoch_loss = total_loss / len(train_loader)\n",
    "    print(f'Epoch {e+1}/{no_of_epochs} | Loss: {epoch_loss:.4f} | Accuracy: {epoch_accuracy:.2f}%')"
   ],
   "id": "7452d05cd712ab82",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 | Loss: 0.1257 | Accuracy: 94.80%\n",
      "Epoch 2/5 | Loss: 0.0509 | Accuracy: 98.20%\n",
      "Epoch 3/5 | Loss: 0.0427 | Accuracy: 98.45%\n",
      "Epoch 4/5 | Loss: 0.0278 | Accuracy: 99.00%\n",
      "Epoch 5/5 | Loss: 0.0257 | Accuracy: 98.96%\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "1e331863fa660dfe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "9802a9e0b9c21cc5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T14:15:30.610961Z",
     "start_time": "2025-07-27T14:13:49.854397Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model.eval()\n",
    "val_correct_preds = 0\n",
    "val_total_data_points = 0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels_batch in test_loader:\n",
    "        images, labels_batch = images.to(device), labels_batch.to(device)\n",
    "\n",
    "        val_total_data_points += images.size(0)\n",
    "        output = model(images)\n",
    "        val_predicted_vals = torch.max(output, dim=1)[1]\n",
    "        val_correct_preds += (val_predicted_vals == labels_batch).sum().item()\n",
    "\n",
    "        all_preds.extend(val_predicted_vals.cpu().numpy())\n",
    "        all_labels.extend(labels_batch.cpu().numpy())\n",
    "\n",
    "val_accuracy = 100 * val_correct_preds / val_total_data_points\n",
    "print(f'Validation Accuracy = {val_accuracy:.2f}%')"
   ],
   "id": "19f1e8bf38a2e898",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy = 98.75%\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T14:22:13.761557Z",
     "start_time": "2025-07-27T14:22:13.667061Z"
    }
   },
   "cell_type": "code",
   "source": "torch.save(model.state_dict(),\"resnet weights.pth\")",
   "id": "23292d2b026945f5",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "80ee45da1b5092b5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T14:22:17.451780Z",
     "start_time": "2025-07-27T14:22:17.416766Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "cr = classification_report(all_labels,all_preds)\n",
    "print(\"--- Confusion Matrix ---\")\n",
    "print(cm)\n",
    "print(\"---Classification Report---\")\n",
    "print(cr)\n"
   ],
   "id": "c4168e907ffb63cf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Confusion Matrix ---\n",
      "[[1314   27]\n",
      " [  38 3837]]\n",
      "---Classification Report---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98      1341\n",
      "           1       0.99      0.99      0.99      3875\n",
      "\n",
      "    accuracy                           0.99      5216\n",
      "   macro avg       0.98      0.99      0.98      5216\n",
      "weighted avg       0.99      0.99      0.99      5216\n",
      "\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T15:27:10.717556Z",
     "start_time": "2025-07-27T15:27:10.061600Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#just testing validating one more time with a small dataset\n",
    "val_dataset = datasets.ImageFolder('C:/Users/visha/Downloads/archive/chest_xray/val',transform=trans)\n",
    "val_loader = DataLoader(val_dataset,batch_size=1,shuffle=False)\n",
    "#set the model to eval\n",
    "model.eval()\n",
    "all_preds =[]\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for images,labels in val_loader:\n",
    "        images,labels = images.to(device),labels.to(images)\n",
    "        output = model(images)\n",
    "        predicted_vals = torch.max(output,dim =1 )[1]\n",
    "        all_preds.extend(predicted_vals.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "print(confusion_matrix(all_preds,all_labels))\n"
   ],
   "id": "48eccab41e982a38",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7 0]\n",
      " [1 8]]\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-27T15:33:51.779339Z",
     "start_time": "2025-07-27T15:33:51.682955Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from PIL import Image\n",
    "\n",
    "model.eval()\n",
    "filepath = 'img.png'\n",
    "\n",
    "image = Image.open(filepath)\n",
    "image = trans(image)\n",
    "image = image.unsqueeze(0)\n",
    "\n",
    "image = image.to(device)\n",
    "output = model(image)\n",
    "predicted = torch.max(output,dim = 1)[1].item()\n",
    "print(predicted)"
   ],
   "id": "e2a0e8e3f8c5eb5b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "execution_count": 59
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-04T15:17:38.084238Z",
     "start_time": "2025-08-04T15:17:38.078448Z"
    }
   },
   "cell_type": "code",
   "source": "print(2+True)",
   "id": "372bb0337dede4f7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3e8642a8249833db"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
